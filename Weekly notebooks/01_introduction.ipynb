{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction\n",
    "\n",
    "Welcome to this first hands-on tutorial! In this notebook, we’ll explore how to train a neural network for image classification using **PyTorch**, a popular deep learning framework. Our dataset of choice is the **CIFAR-10 dataset**, a well-known collection of 60,000 color images (32x32 pixels each) across 10 categories, such as airplanes, cats, and cars. Each category contains 6,000 images, providing a rich dataset for learning.\n",
    "\n",
    "For this task, we will build a **Multi-Layer Perceptron (MLP)**, a foundational type of artificial neural network. An MLP consists of:\n",
    "\n",
    "- **Input Layer**: Processes the raw image data.\n",
    "- **Hidden Layers**: Extract meaningful patterns and features.\n",
    "- **Output Layer**: Maps the learned features to one of the 10 image classes.\n",
    "\n",
    "The connections between nodes in these layers are weighted, and the network learns to adjust these weights during training to improve classification accuracy.\n",
    "\n",
    "By the end of this notebook, you’ll understand how to:\n",
    "1. Preprocess image data for neural network training.\n",
    "2. Build and train an MLP using PyTorch.\n",
    "3. Evaluate the performance of the trained model.\n",
    "\n",
    "Let’s dive in and start building!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries and load the CIFAR-10 dataset\n",
    "\n",
    "In this step, we’ll prepare our tools and data to kickstart the project. First, we’ll import the necessary libraries that make it possible to handle data, build and train neural networks, and visualize results effectively. \n",
    "\n",
    "Next, we’ll load the **CIFAR-10 dataset** using `torchvision`. As a quick refresher, the CIFAR-10 dataset contains 60,000 color images (32x32 pixels) evenly distributed across 10 classes (e.g., airplanes, cats, and cars). Each class has 6,000 images, providing a balanced and diverse dataset for training.\n",
    "\n",
    "To ensure optimal model performance, we’ll also **normalize** the images. Normalization scales the pixel values of the images to a similar range, typically around 0, making it easier for the model to learn and speeding up the convergence process during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    ToTensor,\n",
    "    Normalize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(\n",
    "    root='./data', \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    trainset, \n",
    "    batch_size=10,\n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_dataset = CIFAR10(\n",
    "    root='./data', \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    testset, \n",
    "    batch_size=10,\n",
    "    shuffle=False, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s take a moment to **visualize some images from our dataset**. This is a crucial step to better understand the data we’ll be working with. By examining these images, we can gain insights into the patterns, textures, and challenges involved in classifying them correctly. This also helps us appreciate the complexity of the task our model will tackle as it learns to distinguish between the 10 diverse classes in CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # undo normalization\n",
    "    img = img.numpy()\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "train_iterator = iter(train_dataloader)\n",
    "images, labels = next(train_iterator)\n",
    "\n",
    "# show images\n",
    "imshow(make_grid(images, nrow=10))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define a Simple Multi-Layer Perceptron (MLP)\n",
    "\n",
    "Next, we’ll design a simple **Multi-Layer Perceptron (MLP)** to perform image classification on the CIFAR-10 dataset. An MLP is a foundational type of artificial neural network that uses multiple layers to learn patterns in data.\n",
    "\n",
    "An MLP consists of:\n",
    "- **Input Layer**: Receives the raw input data (in our case, (normalized) image pixels).\n",
    "- **Hidden Layers**: Learn meaningful features by transforming input data through weighted connections and activation functions.\n",
    "- **Output Layer**: Maps the learned features to specific classes (10 in this case).\n",
    "\n",
    "Each node (or neuron) in one layer is connected to every node in the next layer through a weighted connection, allowing the network to capture complex relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # TODO: Define your own layers\n",
    "        self.fc1 = None  # replace with your implementation (make sure the input layer is compatible with the CIFAR-10 images with a size of 32*32*3)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = None  # replace with your implementation (make sure the output layer is compatible with the number of classes (n=10) in CIFAR-10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32*3)  # flatten the images\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define a Loss Function and Optimizer  \n",
    "\n",
    "In this step, we’ll define two key components of our model training process: the **loss function** and the **optimizer**.  \n",
    "\n",
    "- The **loss function** quantifies the difference between the model's predictions and the true labels. It provides feedback that guides the model on how to improve its predictions. For our classification task, we’ll use **Cross Entropy Loss**, which is well-suited for multi-class problems like CIFAR-10.  \n",
    "\n",
    "- The **optimizer** is responsible for adjusting the model’s parameters (weights and biases) to minimize the loss function. We’ll use **Stochastic Gradient Descent (SGD)**, a widely-used optimization algorithm that updates parameters incrementally during training to improve performance.  \n",
    "\n",
    "Together, these tools will enable our model to learn effectively from the data and improve its accuracy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define your own loss function\n",
    "def my_loss_function(outputs, labels):\n",
    "    # Implement your own version of CrossEntropyLoss\n",
    "    # Hint: CrossEntropyLoss is a combination of LogSoftmax and NLLLoss\n",
    "    # You might want to use torch.log and torch.exp functions\n",
    "\n",
    "    # Calculate softmax of the outputs\n",
    "    softmax_outputs = torch.exp(outputs) / torch.sum(torch.exp(outputs), dim=1, keepdim=True)\n",
    "    # Calculate negative log likelihood\n",
    "    neg_log_likelihood = -torch.log(softmax_outputs[range(labels.shape[0]), labels])\n",
    "\n",
    "    # TODO: Complete the implementation by averaging the negative log likelihood across the batch to get the final loss\n",
    "    avg_neg_log_likelihood = None\n",
    "\n",
    "    return avg_neg_log_likelihood\n",
    "\n",
    "\n",
    "criterion = my_loss_function\n",
    "optimizer = SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train the Network and Visualize the Loss  \n",
    "\n",
    "Now, it’s time to train our neural network. **Training** involves optimizing the model’s parameters (weights and biases) to minimize the loss function, allowing the network to make better predictions over time.  \n",
    "\n",
    "The training process consists of the following key steps:  \n",
    "1. **Forward Pass**: The training data is passed through the network, and predictions are generated.  \n",
    "2. **Loss Calculation**: The difference between the model’s predictions and the true labels is measured using the loss function.  \n",
    "3. **Backward Pass**: Gradients of the loss with respect to each parameter are calculated through backpropagation, allowing the model to learn how to adjust its parameters.  \n",
    "4. **Parameter Update**: The optimizer updates the model’s parameters in the direction that reduces the loss, using the gradients calculated in the backward pass.  \n",
    "\n",
    "We’ll repeat this process for several **epochs** (complete passes through the training data). After each epoch, we’ll visualize the loss to track how well the network is learning. A decreasing loss indicates that the model is improving its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "loss_values = []  # to store loss values\n",
    "for epoch in range(40):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # add the loss to running_loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # calculate average loss for this epoch\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    print('Epoch [%d] loss: %.3f' % (epoch + 1, avg_loss))\n",
    "    loss_values.append(avg_loss)  # store the loss value\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel(\"batches\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Test the Network on Test Data and Visualize the Results  \n",
    "\n",
    "Finally, it’s time to evaluate how well our trained network performs on unseen data. This step is critical because it provides an estimate of how the model will generalize to real-world scenarios.  \n",
    "\n",
    "Here’s what we’ll do:  \n",
    "1. **Testing the Network**: We’ll feed the test dataset (data the model hasn’t seen during training) into the network and measure its accuracy in classifying the images. This step ensures that the network hasn’t simply memorized the training data but has learned to generalize to new inputs.  \n",
    "\n",
    "2. **Performance Metrics**: We'll calculate metrics such as accuracy to quantify how well the model distinguishes between the 10 CIFAR-10 classes.  \n",
    "\n",
    "3. **Visualizing Results**: To gain deeper insights into the network’s behavior, we’ll visualize some test images along with their predicted labels. This will help us spot patterns, identify classes the model struggles with, and better understand its performance.  \n",
    "\n",
    "Testing the network is the ultimate check on whether our training efforts were successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_iterator = iter(test_dataloader)\n",
    "images, labels = next(test_iterator)\n",
    "\n",
    "# print images\n",
    "imshow(make_grid(images, nrow=10))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(10)))\n",
    "\n",
    "outputs = net(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(10)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
